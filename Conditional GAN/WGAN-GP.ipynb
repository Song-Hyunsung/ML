{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c51f25-e8b2-4583-b4e3-29c09939f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73ca54-ba9f-401d-808e-511bbb8fcd8d",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "## Discriminator Class\n",
    "- A convolutional neural network designed to classify images as real or fake.\n",
    "- Takes in an image and outputs a probability value indicating whether the image is real or fake.\n",
    "- Uses convolutional layers with **LeakyReLU** activations and **BatchNorm** layers.\n",
    "- The final output is a single probability (between 0 and 1) for each image.\n",
    "\n",
    "## Generator Class\n",
    "- A convolutional neural network that generates synthetic images from random noise vectors.\n",
    "- Takes in a latent vector (usually a random noise vector) and outputs an image.\n",
    "- Uses transposed convolution layers to upsample the latent vector into an image of the desired size.\n",
    "- Outputs images normalized between [-1, 1] using the **Tanh** activation function.\n",
    "\n",
    "## Initialize Weights\n",
    "- A function to initialize the weights of the model layers using **normal distribution** (mean = 0, std = 0.02).\n",
    "- The **Convolutional layers** (`Conv2d` and `ConvTranspose2d`) and **Batch Normalization layers** (`BatchNorm2d`) are initialized to improve training stability.\n",
    "- For **BatchNorm layers**, the scaling factor (`gamma`) is initialized using **normal distribution** with mean = 1 and std = 0.02 to allow slight flexibility in activation scaling.\n",
    "- The **bias** of BatchNorm layers is initialized to 0 to prevent any initial shift in activations.\n",
    "\n",
    "## Gradient Penalty\n",
    "- Gradient Penalty is used to enforce Lipshitz constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2abcfd-20ec-4f26-b350-6f675aa63ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        \"\"\"\n",
    "        Implements the Discriminator for DCGAN with conditional inputs.\n",
    "        The model follows a convolutional architecture that progressively reduces spatial dimensions\n",
    "        while increasing the feature depth. The final output is a single scalar indicating the probability\n",
    "        that the input image is real or fake.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        channels_img : int\n",
    "            Number of channels in the input image.\n",
    "            (For RGB images, this is typically 3. For grayscale images like MNIST, it's 1.)\n",
    "        features_d : int\n",
    "            Number of feature maps in the first convolutional layer.\n",
    "            This number increases in deeper layers to capture more complex features.\n",
    "        num_classes : int\n",
    "            Number of classes in the dataset (used for conditional embedding).\n",
    "        img_size : int\n",
    "            Height and width of the input images (assumed square images: img_size x img_size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Learnable embedding table to represent each class as an img_size x img_size feature map\n",
    "        # Embedding is class-instance (per model) look up table for each classes\n",
    "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: Initial Convolution\n",
    "            # Input: (N, channels_img + 1, img_size, img_size)  [Extra channel from embedding]\n",
    "            # Output: (N, features_d, img_size/2, img_size/2)\n",
    "            nn.Conv2d(\n",
    "                channels_img + 1,  # Input channels: original image + class embedding\n",
    "                features_d,         # Output feature maps\n",
    "                kernel_size=4,      # 4x4 convolution kernel\n",
    "                stride=2,           # Reduces spatial dimensions by half\n",
    "                padding=1           # Maintains proper output size\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),  # LeakyReLU activation with negative slope 0.2\n",
    "\n",
    "            # Layer 2: Downsampling (img_size/2 -> img_size/4)\n",
    "            self._convolutionBlock(features_d, features_d * 2, 4, 2, 1),\n",
    "\n",
    "            # Layer 3: Downsampling (img_size/4 -> img_size/8)\n",
    "            self._convolutionBlock(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "\n",
    "            # Layer 4: Downsampling (img_size/8 -> img_size/16)\n",
    "            self._convolutionBlock(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "\n",
    "            # Final Layer: Fully Connected Convolution\n",
    "            # Input: (N, features_d*8, img_size/16, img_size/16)\n",
    "            # Output: (N, 1, 1, 1) - Single score per image\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _convolutionBlock(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Defines a convolutional block with:\n",
    "        - A 2D Convolution\n",
    "        - Instance Normalization (stabilizes training by normalizing feature maps)\n",
    "        - LeakyReLU Activation (avoids dead neurons)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output feature maps\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel\n",
    "        stride : int\n",
    "            Stride of the convolution (typically 2 for downsampling)\n",
    "        padding : int\n",
    "            Padding applied to the convolution (typically 1 to maintain proper dimensions)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        nn.Sequential : A sequential block of operations\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,  # Bias is removed as InstanceNorm handles normalization\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),  # Normalizes feature maps to stabilize training\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Discriminator.\n",
    "        Labels are first converted to an embedding, reshaped, and concatenated with the image.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input image tensor of shape (N, channels_img, img_size, img_size)\n",
    "        labels : torch.Tensor\n",
    "            Class labels of shape (N,) mapped to embeddings\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (N, 1, 1, 1), representing probability scores of being real/fake.\n",
    "        \"\"\"\n",
    "        # Convert labels into a spatial feature map (N, 1, img_size, img_size)\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        \n",
    "        # Concatenate image with class embedding as an additional channel\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        \n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81797d1-7241-4e51-bbb4-be57bcd8fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g, num_classes, img_size, embed_size):\n",
    "        \"\"\"\n",
    "        Implements the Generator for DCGAN.\n",
    "        The Generator takes a random noise vector (latent space) and transforms it \n",
    "        into a realistic-looking image through a series of transposed convolutions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z_dim : int\n",
    "            Dimension of the latent noise vector (typically 100 in DCGAN implementations).\n",
    "        channels_img : int\n",
    "            Number of channels in the generated image.\n",
    "            (For RGB images, this is typically 3. For grayscale images, it's 1.)\n",
    "        features_g : int\n",
    "            Number of feature maps in the first transposed convolutional layer.\n",
    "            This number scales down in deeper layers to generate finer details.\n",
    "        num_classes : int\n",
    "            Number of distinct classes (e.g., for conditional GANs). This is used to create the \n",
    "            embedding layer to condition the generation on class labels.\n",
    "        img_size : int\n",
    "            The size of the generated image (typically 64 for DCGAN). This determines the output size \n",
    "            after all transposed convolutions and defines the final image dimensions (img_size x img_size).\n",
    "        embed_size : int\n",
    "            Size of the embedding for class labels. This represents how much information from the class \n",
    "            label is encoded and passed into the generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # ---------------------------------------\n",
    "            # Layer 1: Transform Noise Vector (z_dim) into Feature Maps\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, z_dim, 1, 1)  [Latent space input]\n",
    "            # Output: (N, features_g*16, 4, 4)\n",
    "            # Explanation:\n",
    "            # - Converts the 1x1 noise vector into a 4x4 feature map.\n",
    "            # - `stride=1` and `padding=0` ensure the output starts as exactly 4x4.\n",
    "            self._convolutionBlock(z_dim+embed_size, features_g*16, kernel_size=4, stride=1, padding=0),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 2: Upsample to 8x8\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*16, 4, 4)\n",
    "            # Output: (N, features_g*8, 8, 8)\n",
    "            self._convolutionBlock(features_g*16, features_g*8, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 3: Upsample to 16x16\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*8, 8, 8)\n",
    "            # Output: (N, features_g*4, 16, 16)\n",
    "            self._convolutionBlock(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 4: Upsample to 32x32\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*4, 16, 16)\n",
    "            # Output: (N, features_g*2, 32, 32)\n",
    "            self._convolutionBlock(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Final Layer: Upsample to 64x64 (Target Image Size)\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*2, 32, 32)\n",
    "            # Output: (N, channels_img, 64, 64) [Final generated image]\n",
    "            # Explanation:\n",
    "            # - The final layer **does not use BatchNorm** (per the DCGAN paper).\n",
    "            # - Uses `Tanh` activation to output values in [-1, 1] to match normalized image range.\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2,\n",
    "                out_channels=channels_img,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def _convolutionBlock(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Defines a transposed convolutional block used in the Generator.\n",
    "        Each block consists of:\n",
    "        - A Transposed Convolution (upsampling operation)\n",
    "        - Batch Normalization (to stabilize training)\n",
    "        - ReLU Activation (for non-linearity)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input feature maps.\n",
    "        out_channels : int\n",
    "            Number of output feature maps.\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel.\n",
    "        stride : int\n",
    "            Stride of the transposed convolution (typically 2 for upsampling).\n",
    "        padding : int\n",
    "            Padding applied to the transposed convolution (typically 1 for proper output size).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        nn.Sequential : A sequential block of operations.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False  # No bias since BatchNorm is used.\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),  # Stabilizes training by normalizing activations.\n",
    "            nn.ReLU()  # Activation function to introduce non-linearity.\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Generator.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input noise vector of shape (N, z_dim, 1, 1).\n",
    "        labels : torch.Tensor\n",
    "            Tensor of shape (N,) containing class labels for conditional generation.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Output image tensor of shape (N, channels_img, 64, 64).\n",
    "        \"\"\"\n",
    "        # Embed the class labels and concatenate with the input noise vector\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        \n",
    "        # Pass the concatenated vector through the model\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a1a639-0420-46a3-904f-55f1673e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            # Per DCGAN paper, we initialize weights from a normal distribution with mean=0, std=0.02\n",
    "            # This helps stabilize training and prevents mode collapse in GANs.\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02) \n",
    "\n",
    "            # For batch normalization layers, these are default values but explicitly stating it again\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                # Gamma (scaling factor) is initialized to follow N(1, 0.02) per DCGAN recommendations\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)  \n",
    "\n",
    "                # Beta (bias) is set to zero, ensuring the initial batch normalization does not shift activations\n",
    "                nn.init.constant_(m.bias.data, 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6959a6-c6b8-4fbd-bd79-7db009141410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, labels, real, fake, device):\n",
    "    \"\"\"\n",
    "    Computes the gradient penalty for enforcing the Lipschitz constraint in Wasserstein GANs with Gradient Penalty (WGAN-GP).\n",
    "    \n",
    "    Args:\n",
    "        critic (nn.Module): The critic (discriminator) network that maps images to real-valued scores.\n",
    "        real (torch.Tensor): A batch of real images with shape (batch_size, channels, height, width).\n",
    "        fake (torch.Tensor): A batch of generated (fake) images with the same shape as `real`.\n",
    "        device (torch.device): The device (CPU or GPU) where computations should be performed.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The gradient penalty term, a scalar tensor encouraging the gradient norm to be close to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract shape parameters\n",
    "    batch_size, channel_size, height, width = real.shape  # Expecting a 4D tensor: (N, C, H, W)\n",
    "    \n",
    "    # Generate a random interpolation factor `epsilon` for each sample in the batch.\n",
    "    # Shape: (batch_size, 1, 1, 1) -> Broadcasting ensures uniform weighting across all channels & spatial dimensions.\n",
    "    epsilon = torch.rand((batch_size, 1, 1, 1), device=device).repeat(1, channel_size, height, width)  # (N, C, H, W)\n",
    "\n",
    "    # Compute interpolated images as a convex combination of real and fake samples\n",
    "    # Each interpolated image is: ε * real + (1 - ε) * fake\n",
    "    interpolated_images = (real * epsilon) + (fake * (1 - epsilon))  # (N, C, H, W)\n",
    "\n",
    "    # Pass interpolated images through the critic to obtain output scores\n",
    "    mixed_scores = critic(interpolated_images, labels)  # (N, 1) assuming critic outputs a single value per image\n",
    "\n",
    "    # Compute gradients of the critic scores w.r.t. the interpolated images\n",
    "    # grad now holds ∂critic/∂interpolated_images with shape (N, C, H, W)\n",
    "    grad = torch.autograd.grad(\n",
    "        inputs=interpolated_images,    # Input tensor (N, C, H, W)\n",
    "        outputs=mixed_scores,          # Scalar output tensor per sample (N, 1)\n",
    "        grad_outputs=torch.ones_like(mixed_scores),  # Backpropagation signal, same shape as `mixed_scores` (N, 1)\n",
    "        create_graph=True,             # Enables higher-order gradients (for penalty term calculation)\n",
    "        retain_graph=True,             # Retains computation graph for further operations\n",
    "    )[0] # Grad returns tuple size equal to number of inputs, for each gradient w.r.t. each inputs\n",
    "\n",
    "    # Reshape gradient tensor to collapse all non-batch dimensions into a single vector per sample\n",
    "    grad = grad.view(grad.shape[0], -1)  # (N, C*H*W)\n",
    "\n",
    "    # Compute L2 norm of gradients per sample along feature dimension\n",
    "    grad_norm = grad.norm(2, dim=1)  # (N,)\n",
    "\n",
    "    # Compute the gradient penalty term, enforcing ||∇D(interpolated_images)|| ≈ 1\n",
    "    grad_penalty = torch.mean((grad_norm - 1) ** 2)  # Scalar tensor\n",
    "\n",
    "    return grad_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57d711-fe57-40e6-b1c8-5e7ebf68f077",
   "metadata": {},
   "source": [
    "# Conditional Wasserstein GAN with Gradient Penalty on CelebA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4eba71-16eb-4d72-8318-e55bae68dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters for training\n",
    "lr = 1e-4\n",
    "batch_size = 64 \n",
    "img_size = 64\n",
    "img_channels = 1\n",
    "num_classes = 10\n",
    "generator_embedding = 100\n",
    "z_dim = 100\n",
    "num_epochs = 5\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "critic_iterations = 5\n",
    "lambda_penalty = 10\n",
    "\n",
    "# Data transformations for preprocessing the CELEB dataset\n",
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        # Resize images to the specified size (img_size x img_size)\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the images so pixel values are between [-1, 1]\n",
    "        transforms.Normalize([0.5 for _ in range(img_channels)], [0.5 for _ in range(img_channels)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the MNIST dataset with the specified transformations\n",
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transformer, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Initialize the generator and discriminator models and move them to the appropriate device (GPU or CPU)\n",
    "gen_model = Generator(z_dim, img_channels, features_gen, num_classes, img_size, generator_embedding).to(device)\n",
    "disc_model = Discriminator(img_channels, features_disc, num_classes, img_size).to(device)\n",
    "\n",
    "# Initialize the weights of both the generator and discriminator using the custom function\n",
    "initialize_weights(gen_model)\n",
    "initialize_weights(disc_model)\n",
    "\n",
    "# Set up Adam optimizers for both models with the same learning rate and betas for stability in training\n",
    "optimizer_gen = optim.Adam(gen_model.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "optimizer_disc = optim.Adam(disc_model.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "\n",
    "# Set up TensorBoard writers to log images for both real and fake images\n",
    "writer_fake = SummaryWriter(f\"runs/DCGAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/DCGAN_MNIST/real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c2b597-08bd-454b-be7b-4999d974a542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5797), started 0:07:52 ago. (Use '!kill 5797' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1f520752f7a0d59f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1f520752f7a0d59f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard is running on port 6006\n",
      "Epoch [0/5] Batch 0/937                   Loss D: -12.6377, loss G: 9.1760\n",
      "Epoch [0/5] Batch 100/937                   Loss D: -19.6038, loss G: 63.0470\n",
      "Epoch [0/5] Batch 200/937                   Loss D: -15.8541, loss G: 78.5505\n",
      "Epoch [0/5] Batch 300/937                   Loss D: -13.1848, loss G: 73.0322\n",
      "Epoch [0/5] Batch 400/937                   Loss D: -12.3208, loss G: 83.5074\n",
      "Epoch [0/5] Batch 500/937                   Loss D: -14.2987, loss G: 76.1848\n",
      "Epoch [0/5] Batch 600/937                   Loss D: -12.0301, loss G: 71.2471\n",
      "Epoch [0/5] Batch 700/937                   Loss D: -8.8279, loss G: 69.6128\n",
      "Epoch [0/5] Batch 800/937                   Loss D: -8.0103, loss G: 65.7421\n",
      "Epoch [0/5] Batch 900/937                   Loss D: -9.6818, loss G: 59.8363\n",
      "Epoch [1/5] Batch 0/937                   Loss D: -8.6036, loss G: 58.8834\n",
      "Epoch [1/5] Batch 100/937                   Loss D: -8.5183, loss G: 61.4852\n",
      "Epoch [1/5] Batch 200/937                   Loss D: -7.4263, loss G: 61.7930\n",
      "Epoch [1/5] Batch 300/937                   Loss D: -8.4653, loss G: 57.7023\n",
      "Epoch [1/5] Batch 400/937                   Loss D: -8.3801, loss G: 54.7986\n",
      "Epoch [1/5] Batch 500/937                   Loss D: -7.1049, loss G: 51.9688\n",
      "Epoch [1/5] Batch 600/937                   Loss D: -6.7143, loss G: 50.7528\n",
      "Epoch [1/5] Batch 700/937                   Loss D: -6.6113, loss G: 49.7603\n",
      "Epoch [1/5] Batch 800/937                   Loss D: -7.2662, loss G: 47.0583\n",
      "Epoch [1/5] Batch 900/937                   Loss D: -7.1380, loss G: 44.2029\n",
      "Epoch [2/5] Batch 0/937                   Loss D: -5.6937, loss G: 31.1864\n",
      "Epoch [2/5] Batch 100/937                   Loss D: -8.9196, loss G: 34.4260\n",
      "Epoch [2/5] Batch 200/937                   Loss D: -4.8163, loss G: 39.1411\n",
      "Epoch [2/5] Batch 300/937                   Loss D: -8.0706, loss G: 38.2737\n",
      "Epoch [2/5] Batch 400/937                   Loss D: -6.5164, loss G: 45.6835\n",
      "Epoch [2/5] Batch 500/937                   Loss D: -9.0469, loss G: 42.8586\n",
      "Epoch [2/5] Batch 600/937                   Loss D: -7.5539, loss G: 31.5843\n",
      "Epoch [2/5] Batch 700/937                   Loss D: -8.3600, loss G: 29.1049\n",
      "Epoch [2/5] Batch 800/937                   Loss D: -5.9379, loss G: 26.9980\n",
      "Epoch [2/5] Batch 900/937                   Loss D: -4.9195, loss G: 26.8840\n",
      "Epoch [3/5] Batch 0/937                   Loss D: -7.7769, loss G: 23.3812\n",
      "Epoch [3/5] Batch 100/937                   Loss D: -10.2930, loss G: 28.2617\n",
      "Epoch [3/5] Batch 200/937                   Loss D: -7.2967, loss G: 15.2770\n",
      "Epoch [3/5] Batch 300/937                   Loss D: -4.6852, loss G: 10.0937\n",
      "Epoch [3/5] Batch 400/937                   Loss D: -6.4411, loss G: 21.9214\n",
      "Epoch [3/5] Batch 500/937                   Loss D: -8.2840, loss G: 10.4604\n",
      "Epoch [3/5] Batch 600/937                   Loss D: -4.7963, loss G: 3.4560\n",
      "Epoch [3/5] Batch 700/937                   Loss D: -5.9735, loss G: 22.7910\n",
      "Epoch [3/5] Batch 800/937                   Loss D: -6.0740, loss G: -0.3429\n",
      "Epoch [3/5] Batch 900/937                   Loss D: -5.3022, loss G: -7.2419\n",
      "Epoch [4/5] Batch 0/937                   Loss D: -4.5547, loss G: -5.1774\n",
      "Epoch [4/5] Batch 100/937                   Loss D: -4.5724, loss G: 3.3365\n",
      "Epoch [4/5] Batch 200/937                   Loss D: -7.6952, loss G: -5.7717\n",
      "Epoch [4/5] Batch 300/937                   Loss D: -8.8609, loss G: -10.6536\n",
      "Epoch [4/5] Batch 400/937                   Loss D: -6.8521, loss G: -4.0450\n",
      "Epoch [4/5] Batch 500/937                   Loss D: -5.8757, loss G: 1.6956\n",
      "Epoch [4/5] Batch 600/937                   Loss D: -5.2581, loss G: -8.4056\n",
      "Epoch [4/5] Batch 700/937                   Loss D: -6.1726, loss G: -17.8644\n",
      "Epoch [4/5] Batch 800/937                   Loss D: -7.1815, loss G: -26.0339\n",
      "Epoch [4/5] Batch 900/937                   Loss D: -7.3101, loss G: -16.9761\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=runs/DCGAN_MNIST --bind_all --port=6006\n",
    "print(\"Tensorboard is running on port 6006\")\n",
    "\n",
    "# Set the models to training mode\n",
    "gen_model.train()\n",
    "disc_model.train()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, labels) in enumerate(loader):\n",
    "        # Move the real images to the device (GPU/CPU)\n",
    "        real = real.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # --- Train the critic ---\n",
    "        for _ in range(critic_iterations):\n",
    "            # Generate random noise to feed into the generator\n",
    "            noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        \n",
    "            # Generate fake images using the generator\n",
    "            fake = gen_model(noise, labels)\n",
    "        \n",
    "            # Compute the discriminator's output on real images\n",
    "            disc_real = disc_model(real, labels).reshape(-1)\n",
    "            \n",
    "            # Compute the discriminator's output on fake images\n",
    "            disc_fake = disc_model(fake.detach(), labels).reshape(-1)\n",
    "\n",
    "            # We want to maximize this, so negative the minimization along with gradient penalty\n",
    "            penalty = gradient_penalty(disc_model, labels, real, fake, device)\n",
    "            loss_disc = (-(torch.mean(disc_real) - torch.mean(disc_fake))) + (lambda_penalty * penalty)\n",
    "        \n",
    "            disc_model.zero_grad()\n",
    "            loss_disc.backward(retain_graph=True)\n",
    "            optimizer_disc.step()\n",
    "\n",
    "        # --- Train the Generator ---\n",
    "        # Compute the discriminator's output on the fake images\n",
    "        output = disc_model(fake, labels).reshape(-1)\n",
    "        \n",
    "        # min -E[critic(gen_fake)]\n",
    "        loss_gen = -torch.mean(output)\n",
    "        \n",
    "        gen_model.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Print losses and log images to TensorBoard every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Log real and fake images to TensorBoard\n",
    "            with torch.no_grad():  # No gradients are needed for this step\n",
    "                fake = gen_model(noise, labels)\n",
    "                # Create image grids for real and fake images (up to 32 images)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                # Add images to TensorBoard (real and fake images for visualization)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e13d2-b9d3-4bf3-9ffc-411d9767e937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
