{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c340855-be78-4d9e-b383-f7ccede7adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5744bcc-9974-4b92-8e92-ac2fb10475f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified image encoder that maps a 256x256 input image to a latent representation,\n",
    "    and also returns a skip connection feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim=256):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        # Define the encoder body up to the skip connection extraction:\n",
    "        self.encoder_body = nn.Sequential(\n",
    "            # First block: 256x256 -> 128x128\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),    # [B, 64, 256, 256]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                # [B, 64, 128, 128]\n",
    "            \n",
    "            # Second block: 128x128 -> 64x64\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),   # [B, 128, 128, 128]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                # [B, 128, 64, 64]\n",
    "            \n",
    "            # Third block: 64x64 -> 32x32\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # [B, 256, 64, 64]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                # [B, 256, 32, 32]\n",
    "            \n",
    "            # EXTRA block: deeper feature extraction at 32x32 resolution\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # [B, 256, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # [B, 256, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Projection to embedding dimension â€“ produces skip feature map\n",
    "            nn.Conv2d(256, emb_dim, kernel_size=3, padding=1),  # [B, emb_dim, 32, 32]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Global pooling to generate latent vector from the skip feature map.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)  # [B, emb_dim, 1, 1]\n",
    "\n",
    "    def forward(self, od_image, return_skip=False):\n",
    "        # Run the encoder body to get the feature map (skip connection)\n",
    "        x = self.encoder_body(od_image)  # shape: [B, emb_dim, 32, 32]\n",
    "        # Pool to obtain the latent vector\n",
    "        latent = self.avgpool(x).squeeze(-1).squeeze(-1)  # shape: [B, emb_dim]\n",
    "        if return_skip:\n",
    "            return latent, x  # Return both latent vector and skip connection feature map\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67af041f-b64a-4afc-879c-b08d17f9ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder that reconstructs a 256x256 RGB image from a latent vector,\n",
    "    incorporating a skip connection from the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=256, emb_dim=256):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        # Project latent vector to a feature map of shape [B, emb_dim, 32, 32]\n",
    "        self.fc = nn.Linear(latent_dim, emb_dim * 32 * 32)\n",
    "        \n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            # Upsample from 32x32 -> 64x64\n",
    "            nn.ConvTranspose2d(emb_dim, 128, kernel_size=4, stride=2, padding=1),  # [B, 128, 64, 64]\n",
    "            nn.ReLU(),\n",
    "            # Extra convolution block at resolution 64x64\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Upsample from 64x64 -> 128x128\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),       # [B, 64, 128, 128]\n",
    "            nn.ReLU(),\n",
    "            # Extra convolution block at resolution 128x128\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Upsample from 128x128 -> 256x256\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),        # [B, 32, 256, 256]\n",
    "            nn.ReLU(),\n",
    "            # Extra convolution block at resolution 256x256\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Final convolution: map to RGB channels\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Use Tanh if your images are normalized to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, latent, skip_connection=None):\n",
    "        # Project the latent vector back into a spatial feature map.\n",
    "        x = self.fc(latent)\n",
    "        x = x.view(x.size(0), -1, 32, 32)\n",
    "        # Incorporate the skip connection via element-wise addition if provided.\n",
    "        if skip_connection is not None:\n",
    "            x = x + skip_connection\n",
    "        x = self.deconv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af5b7a7-ad62-4ff9-bdba-40d1f9038367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder that combines the ImageEncoder and ImageDecoder with skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=256, emb_dim=256):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = ImageEncoder(emb_dim=emb_dim)\n",
    "        self.decoder = ImageDecoder(latent_dim=latent_dim, emb_dim=emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Return both the latent vector and the intermediate skip feature map.\n",
    "        latent, skip = self.encoder(x, return_skip=True)\n",
    "        # Pass the skip connection into the decoder.\n",
    "        recon = self.decoder(latent, skip_connection=skip)\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "832e6b00-7641-4ba9-abe7-218126b57c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinalImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        # List all image files in the directory (you can adjust the extensions as needed)\n",
    "        self.image_files = [os.path.join(image_dir, f)\n",
    "                            for f in os.listdir(image_dir)\n",
    "                            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read the image using PIL\n",
    "        image_path = self.image_files[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb90a33-9aee-482d-b57e-00a30284d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a VGG-based feature extractor.\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, layer_index=9):\n",
    "        \"\"\"\n",
    "        Extract features from the VGG16 network up to a certain layer.\n",
    "        :param layer_index: Index up to which to extract features (e.g., 9 corresponds to 'relu2_2').\n",
    "        \"\"\"\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        vgg_pretrained = models.vgg16(pretrained=True).features\n",
    "        # Build a sequential module from the first layer up to layer_index\n",
    "        self.features = nn.Sequential(*[vgg_pretrained[x] for x in range(layer_index)])\n",
    "        # Freeze parameters to avoid updates during training\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # VGG normalization values (if needed)\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
    "        self.std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalize input as VGG expects values in [0, 1] normalized with these mean & std.\n",
    "        x = (x - self.mean.to(x.device)) / self.std.to(x.device)\n",
    "        return self.features(x)\n",
    "\n",
    "# Perceptual loss function (using L1 loss between feature maps)\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.criterion = nn.L1Loss()  # alternatively, nn.MSELoss()\n",
    "        \n",
    "    def forward(self, recon, target):\n",
    "        # Extract features of both reconstructed and target images.\n",
    "        features_recon = self.feature_extractor(recon)\n",
    "        features_target = self.feature_extractor(target)\n",
    "        # Compute the loss on features\n",
    "        loss = self.criterion(features_recon, features_target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b931e0-2604-4014-83ac-1ee04588b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = AutoEncoder(latent_dim=128, emb_dim=128).to(device)\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Create the VGG feature extractor (extracting up to layer 9)\n",
    "vgg_extractor = VGGFeatureExtractor(layer_index=9).to('cuda')\n",
    "perceptual_loss_fn = PerceptualLoss(vgg_extractor).to('cuda')\n",
    "reconstruction_loss_fn = nn.L1Loss()  # or nn.MSELoss() for pixel space loss\n",
    "\n",
    "# Define the transforms (e.g., resize to 256x256, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Directory with retinal images\n",
    "image_dir = './dataset/train'\n",
    "val_image_dir = './dataset/val'\n",
    "\n",
    "dataset = RetinalImageDataset(image_dir, transform=transform)\n",
    "val_dataset = RetinalImageDataset(val_image_dir, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb6375-8a37-4b08-b83b-9a55050a8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5001\n",
    "best_val = float('inf')\n",
    "\n",
    "# If you already have a checkpoint, you can load the saved model and optimizer states.\n",
    "# checkpoint_path = \"./validation_results/saved_models/autoencoder_checkpoint200.pth\"\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    running_train_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon = autoencoder(batch)\n",
    "        \n",
    "        # Compute the reconstruction loss in pixel space (optional or weighted)\n",
    "        rec_loss = reconstruction_loss_fn(recon, batch)\n",
    "        # Compute the perceptual loss\n",
    "        perc_loss = perceptual_loss_fn(recon, batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        # Combine losses (weights can be tuned)\n",
    "        loss = rec_loss + 2 * perc_loss  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item() * batch.size(0)\n",
    "    \n",
    "    # Compute average training loss for this epoch.\n",
    "    train_loss = running_train_loss / len(dataset)\n",
    "    \n",
    "    # --- Validation phase ---\n",
    "    autoencoder.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, val_batch in enumerate(val_dataloader):\n",
    "            val_batch = val_batch.to(device)\n",
    "            val_recon = autoencoder(val_batch)\n",
    "\n",
    "            val_rec_loss = reconstruction_loss_fn(val_recon, val_batch)\n",
    "            val_perc_loss = perceptual_loss_fn(val_recon, val_batch)\n",
    "            \n",
    "            val_loss = val_rec_loss + 2 * val_perc_loss\n",
    "            \n",
    "            running_val_loss += val_loss.item() * val_batch.size(0)\n",
    "    \n",
    "    val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save a checkpoint when validation loss improves.\n",
    "    if epoch > 1000 and val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        os.makedirs(\"./validation_results/saved_models\", exist_ok=True)\n",
    "        # Save both model and optimizer states.\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': autoencoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, f\"./validation_results/saved_models/autoencoder_checkpoint_epoch{epoch}.pth\")\n",
    "        \n",
    "        os.makedirs(\"validation_results\", exist_ok=True)\n",
    "        # Save one batch of validation reconstructions.\n",
    "        with torch.no_grad():\n",
    "            sample_batch = next(iter(val_dataloader)).to(device)\n",
    "            recon_images = autoencoder(sample_batch)\n",
    "            # Save the reconstructed images as a grid image.\n",
    "            save_image(recon_images, f\"validation_results/reconstructed_val_epoch{epoch}.png\", nrow=4, normalize=True)\n",
    "            # Also save the original images for comparison.\n",
    "            save_image(sample_batch, f\"validation_results/original_val_epoch{epoch}.png\", nrow=4, normalize=True)\n",
    "        \n",
    "        print(\"Saved model and optimizer checkpoint and validation images.\")\n",
    "    # Also save every 200 epochs even if not improved.\n",
    "    elif epoch % 200 == 0:\n",
    "        os.makedirs(\"./validation_results/saved_models\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': autoencoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, f\"./validation_results/saved_models/autoencoder_checkpoint_epoch{epoch}.pth\")\n",
    "        \n",
    "        os.makedirs(\"validation_results\", exist_ok=True)\n",
    "        with torch.no_grad():\n",
    "            sample_batch = next(iter(val_dataloader)).to(device)\n",
    "            recon_images = autoencoder(sample_batch)\n",
    "            save_image(recon_images, f\"validation_results/reconstructed_val_epoch{epoch}.png\", nrow=4, normalize=True)\n",
    "            save_image(sample_batch, f\"validation_results/original_val_epoch{epoch}.png\", nrow=4, normalize=True)\n",
    "        \n",
    "        print(\"Saved periodic model and optimizer checkpoint and validation images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "180364c8-e94e-45e9-8ff7-8a9d3a72f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset/test/patient_00.jpg', './dataset/test/patient_01.jpg', './dataset/test/patient_02.jpg', './dataset/test/patient_03.jpg', './dataset/test/patient_04.jpg', './dataset/test/patient_05.jpg', './dataset/test/patient_06.jpg', './dataset/test/patient_07.jpg', './dataset/test/patient_08.jpg', './dataset/test/patient_09.jpg', './dataset/test/patient_10.jpg', './dataset/test/patient_11.jpg', './dataset/test/patient_12.jpg', './dataset/test/patient_13.jpg', './dataset/test/patient_14.jpg', './dataset/test/patient_15.jpg', './dataset/test/patient_16.jpg', './dataset/test/patient_17.jpg', './dataset/test/patient_18.jpg', './dataset/test/patient_19.jpg', './dataset/test/patient_20.jpg', './dataset/test/patient_21.jpg', './dataset/test/patient_22.jpg', './dataset/test/patient_23.jpg', './dataset/test/patient_24.jpg', './dataset/test/patient_25.jpg', './dataset/test/patient_26.jpg', './dataset/test/patient_27.jpg', './dataset/test/patient_28.jpg', './dataset/test/patient_29.jpg', './dataset/test/patient_30.jpg', './dataset/test/patient_31.jpg', './dataset/test/patient_32.jpg', './dataset/test/patient_33.jpg', './dataset/test/patient_34.jpg', './dataset/test/patient_35.jpg', './dataset/test/patient_36.jpg', './dataset/test/patient_37.jpg', './dataset/test/patient_38.jpg', './dataset/test/patient_39.jpg', './dataset/test/patient_40.jpg', './dataset/test/patient_41.jpg', './dataset/test/patient_42.jpg', './dataset/test/patient_43.jpg', './dataset/test/patient_44.jpg', './dataset/test/patient_45.jpg', './dataset/test/patient_46.jpg', './dataset/test/patient_47.jpg', './dataset/test/patient_48.jpg', './dataset/test/patient_49.jpg', './dataset/test/patient_50.jpg', './dataset/test/patient_51.jpg', './dataset/test/patient_52.jpg', './dataset/test/patient_53.jpg', './dataset/test/patient_54.jpg', './dataset/test/patient_55.jpg', './dataset/test/patient_56.jpg', './dataset/test/patient_57.jpg', './dataset/test/patient_58.jpg', './dataset/test/patient_59.jpg', './dataset/test/patient_60.jpg', './dataset/test/patient_61.jpg', './dataset/test/patient_62.jpg', './dataset/test/patient_63.jpg']\n"
     ]
    }
   ],
   "source": [
    "# If you already have a checkpoint, you can load the saved model and optimizer states.\n",
    "checkpoint_path = \"./validation_results/saved_models/autoencoder_checkpoint_epoch4400.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "# Directory with retinal images\n",
    "test_image_dir = './dataset/test'\n",
    "test_dataset = RetinalImageDataset(test_image_dir, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Run the autoencoder on the test set\n",
    "with torch.no_grad():\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "        # If your test dataset yields a tuple (input, label) adjust this accordingly:\n",
    "        # inputs, _ = test_data\n",
    "        inputs = test_data.to(device)  # Move inputs to the device\n",
    "        \n",
    "        # Forward pass: generate output from the autoencoder\n",
    "        outputs = autoencoder(inputs)\n",
    "        \n",
    "        save_image(outputs, f\"test_results/patient_{batch_idx}.png\", nrow=1, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "676a07c7-862d-4700-bd5f-efd372ea2fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for patient_00: 0.9444077014923096\n",
      "SSIM for patient_01: 0.9275009036064148\n",
      "SSIM for patient_02: 0.9302933812141418\n",
      "SSIM for patient_03: 0.7561014294624329\n",
      "SSIM for patient_04: 0.9409477114677429\n",
      "SSIM for patient_05: 0.6331303119659424\n",
      "SSIM for patient_06: 0.7453079223632812\n",
      "SSIM for patient_07: 0.7672991752624512\n",
      "SSIM for patient_08: 0.9386633038520813\n",
      "SSIM for patient_09: 0.9399006366729736\n",
      "SSIM for patient_10: 0.41984423995018005\n",
      "SSIM for patient_11: 0.906661331653595\n",
      "SSIM for patient_12: 0.9183079600334167\n",
      "SSIM for patient_13: 0.9610039591789246\n",
      "SSIM for patient_14: 0.44831550121307373\n",
      "SSIM for patient_15: 0.625623881816864\n",
      "SSIM for patient_16: 0.9654307961463928\n",
      "SSIM for patient_17: 0.9540240168571472\n",
      "SSIM for patient_18: 0.9481490254402161\n",
      "SSIM for patient_19: 0.9337709546089172\n",
      "SSIM for patient_20: 0.9026065468788147\n",
      "SSIM for patient_21: 0.9482941627502441\n",
      "SSIM for patient_22: 0.958624541759491\n",
      "SSIM for patient_23: 0.9517131447792053\n",
      "SSIM for patient_24: 0.9480114579200745\n",
      "SSIM for patient_25: 0.3955477476119995\n",
      "SSIM for patient_26: 0.7111856341362\n",
      "SSIM for patient_27: 0.9269866943359375\n",
      "SSIM for patient_28: 0.8636592030525208\n",
      "SSIM for patient_29: 0.8139870762825012\n",
      "SSIM for patient_30: 0.835168182849884\n",
      "SSIM for patient_31: 0.9044498801231384\n",
      "SSIM for patient_32: 0.918236255645752\n",
      "SSIM for patient_33: 0.7445572018623352\n",
      "SSIM for patient_34: 0.900507926940918\n",
      "SSIM for patient_35: 0.9424687027931213\n",
      "SSIM for patient_36: 0.9050909876823425\n",
      "SSIM for patient_37: 0.9562708735466003\n",
      "SSIM for patient_38: 0.7119929194450378\n",
      "SSIM for patient_39: 0.9139975905418396\n",
      "SSIM for patient_40: 0.9404180645942688\n",
      "SSIM for patient_41: 0.951420783996582\n",
      "SSIM for patient_42: 0.7052481770515442\n",
      "SSIM for patient_43: 0.957697868347168\n",
      "SSIM for patient_44: 0.9637729525566101\n",
      "SSIM for patient_45: 0.9347074031829834\n",
      "SSIM for patient_46: 0.9260689616203308\n",
      "SSIM for patient_47: 0.9341332912445068\n",
      "SSIM for patient_48: 0.9543240666389465\n",
      "SSIM for patient_49: 0.6636379361152649\n",
      "SSIM for patient_50: 0.9598737359046936\n",
      "SSIM for patient_51: 0.9178228974342346\n",
      "SSIM for patient_52: 0.9307568669319153\n",
      "SSIM for patient_53: 0.9341846108436584\n",
      "SSIM for patient_54: 0.9136586785316467\n",
      "SSIM for patient_55: 0.9436033368110657\n",
      "SSIM for patient_56: 0.9414541125297546\n",
      "SSIM for patient_57: 0.9230527877807617\n",
      "SSIM for patient_58: 0.8537513613700867\n",
      "SSIM for patient_59: 0.9671764969825745\n",
      "SSIM for patient_60: 0.9607081413269043\n",
      "SSIM for patient_61: 0.9455606937408447\n",
      "SSIM for patient_62: 0.9494883418083191\n",
      "SSIM for patient_63: 0.9204599857330322\n",
      "Average SSIM over 64 images: 0.8711097254417837\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define directories for test and target images.\n",
    "test_dir = \"./test_results_3432/\"\n",
    "target_dir = \"./dataset/test/\"\n",
    "\n",
    "# Define transformation: resizing, converting to tensor, and normalization.\n",
    "# Adjust normalization or data_range as needed:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalization scales pixel values to approximately [-1, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Set data_range based on your transform:\n",
    "# For normalized images (range ~[-1,1]), use data_range=2.0.\n",
    "# If you remove Normalize, then use data_range=1.0.\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=2.0)\n",
    "\n",
    "num_images = 64\n",
    "total_ssim = 0.0\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Construct file names. The format {:02d} ensures two digits (e.g., \"00\", \"01\", \"63\")\n",
    "    test_file = os.path.join(test_dir, f\"patient_{i:02d}.png\")\n",
    "    target_file = os.path.join(target_dir, f\"patient_{i:02d}.jpg\")\n",
    "    \n",
    "    # Open images and convert to RGB\n",
    "    test_img = Image.open(test_file).convert(\"RGB\")\n",
    "    target_img = Image.open(target_file).convert(\"RGB\")\n",
    "    \n",
    "    # Apply transformation and add the batch dimension (shape: (1, C, H, W))\n",
    "    test_tensor = transform(test_img).unsqueeze(0)\n",
    "    target_tensor = transform(target_img).unsqueeze(0)\n",
    "    \n",
    "    # Compute SSIM for the current image pair.\n",
    "    ssim_score = ssim_metric(test_tensor, target_tensor)\n",
    "    print(f\"SSIM for patient_{i:02d}: {ssim_score.item()}\")\n",
    "    \n",
    "    total_ssim += ssim_score.item()\n",
    "\n",
    "average_ssim = total_ssim / num_images\n",
    "print(f\"Average SSIM over {num_images} images: {average_ssim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbbd352-6d17-4f56-bdd7-85249e2a9f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
