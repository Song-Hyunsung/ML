{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c51f25-e8b2-4583-b4e3-29c09939f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73ca54-ba9f-401d-808e-511bbb8fcd8d",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "## Discriminator Class\n",
    "- A convolutional neural network designed to classify images as real or fake.\n",
    "- Takes in an image and outputs a probability value indicating whether the image is real or fake.\n",
    "- Uses convolutional layers with **LeakyReLU** activations and **BatchNorm** layers.\n",
    "- The final output is a single probability (between 0 and 1) for each image.\n",
    "\n",
    "## Generator Class\n",
    "- A convolutional neural network that generates synthetic images from random noise vectors.\n",
    "- Takes in a latent vector (usually a random noise vector) and outputs an image.\n",
    "- Uses transposed convolution layers to upsample the latent vector into an image of the desired size.\n",
    "- Outputs images normalized between [-1, 1] using the **Tanh** activation function.\n",
    "\n",
    "## Initialize Weights\n",
    "- A function to initialize the weights of the model layers using **normal distribution** (mean = 0, std = 0.02).\n",
    "- The **Convolutional layers** (`Conv2d` and `ConvTranspose2d`) and **Batch Normalization layers** (`BatchNorm2d`) are initialized to improve training stability.\n",
    "- For **BatchNorm layers**, the scaling factor (`gamma`) is initialized using **normal distribution** with mean = 1 and std = 0.02 to allow slight flexibility in activation scaling.\n",
    "- The **bias** of BatchNorm layers is initialized to 0 to prevent any initial shift in activations.\n",
    "\n",
    "## Gradient Penalty\n",
    "- Gradient Penalty is used to enforce Lipshitz constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2abcfd-20ec-4f26-b350-6f675aa63ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        \"\"\"\n",
    "        Implements the Discriminator for DCGAN.\n",
    "        This model follows a convolutional architecture that progressively reduces the spatial dimensions \n",
    "        while increasing the feature depth. The final output is a single scalar value (0 or 1) indicating \n",
    "        whether the input image is real or fake.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        channels_img : int\n",
    "            Number of channels in the input image. \n",
    "            (For RGB images, this is typically 3. For grayscale images like MNIST, it's 1.)\n",
    "        features_d : int\n",
    "            Number of feature maps in the first convolutional layer.\n",
    "            This number scales up in deeper layers to capture more complex features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # -------------------------\n",
    "            # Layer 1: Initial Convolution\n",
    "            # -------------------------\n",
    "            # Input: (N, channels_img, 64, 64)\n",
    "            # Output: (N, features_d, 32, 32)\n",
    "            # Explanation:\n",
    "            # - Stride=2 reduces width and height by half\n",
    "            # - No batch normalization in the first layer as per the DCGAN paper\n",
    "            nn.Conv2d(\n",
    "                channels_img,      # Number of input channels (e.g., 3 for RGB images)\n",
    "                features_d,        # Number of output feature maps\n",
    "                kernel_size=4,     # 4x4 convolution kernel\n",
    "                stride=2,          # Reduces spatial dimensions (64x64 -> 32x32)\n",
    "                padding=1          # Maintains proper output size after convolution\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),  # LeakyReLU activation with a negative slope of 0.2\n",
    "\n",
    "            # -------------------------\n",
    "            # Layer 2: Downsampling\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d, 32, 32)\n",
    "            # Output: (N, features_d*2, 16, 16)\n",
    "            self._convolutionBlock(features_d, features_d*2, 4, 2, 1),\n",
    "\n",
    "            # -------------------------\n",
    "            # Layer 3: Downsampling\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d*2, 16, 16)\n",
    "            # Output: (N, features_d*4, 8, 8)\n",
    "            self._convolutionBlock(features_d*2, features_d*4, 4, 2, 1),\n",
    "\n",
    "            # -------------------------\n",
    "            # Layer 4: Downsampling\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d*4, 8, 8)\n",
    "            # Output: (N, features_d*8, 4, 4)\n",
    "            self._convolutionBlock(features_d*4, features_d*8, 4, 2, 1),\n",
    "\n",
    "            # -------------------------\n",
    "            # Final Layer: Fully Connected Convolution\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d*8, 4, 4)\n",
    "            # Output: (N, 1, 1, 1)\n",
    "            # Explanation:\n",
    "            # - This layer performs a final convolution that reduces the spatial dimension to 1x1\n",
    "            # - The output is a single value per image, indicating the probability of being real or fake\n",
    "            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _convolutionBlock(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Defines a convolutional block used in the Discriminator.\n",
    "        Each block consists of:\n",
    "        - A 2D Convolution (with no bias to improve stability)\n",
    "        - Batch Normalization (to stabilize learning)\n",
    "        - LeakyReLU Activation (to allow small gradients even for negative inputs)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output feature maps\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel\n",
    "        stride : int\n",
    "            Stride of the convolution (typically 2 for downsampling)\n",
    "        padding : int\n",
    "            Padding applied to the convolution (typically 1 to maintain proper dimensions)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        nn.Sequential : A sequential block of operations\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,  # Bias is removed as BatchNorm handles normalization\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),  # Normalizes feature maps to stabilize training\n",
    "            nn.LeakyReLU(0.2),  # Allows a small gradient for negative inputs, avoiding dead neurons\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Discriminator.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input image tensor of shape (N, channels_img, 64, 64)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (N, 1, 1, 1), representing probability scores of being real/fake.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81797d1-7241-4e51-bbb4-be57bcd8fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        \"\"\"\n",
    "        Implements the Generator for DCGAN.\n",
    "        The Generator takes a random noise vector (latent space) and transforms it \n",
    "        into a realistic-looking image through a series of transposed convolutions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z_dim : int\n",
    "            Dimension of the latent noise vector (typically 100 in DCGAN implementations).\n",
    "        channels_img : int\n",
    "            Number of channels in the generated image.\n",
    "            (For RGB images, this is typically 3. For grayscale images, it's 1.)\n",
    "        features_g : int\n",
    "            Number of feature maps in the first transposed convolutional layer.\n",
    "            This number scales down in deeper layers to generate finer details.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # ---------------------------------------\n",
    "            # Layer 1: Transform Noise Vector (z_dim) into Feature Maps\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, z_dim, 1, 1)  [Latent space input]\n",
    "            # Output: (N, features_g*16, 4, 4)\n",
    "            # Explanation:\n",
    "            # - Converts the 1x1 noise vector into a 4x4 feature map.\n",
    "            # - `stride=1` and `padding=0` ensure the output starts as exactly 4x4.\n",
    "            self._convolutionBlock(z_dim, features_g*16, kernel_size=4, stride=1, padding=0),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 2: Upsample to 8x8\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*16, 4, 4)\n",
    "            # Output: (N, features_g*8, 8, 8)\n",
    "            self._convolutionBlock(features_g*16, features_g*8, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 3: Upsample to 16x16\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*8, 8, 8)\n",
    "            # Output: (N, features_g*4, 16, 16)\n",
    "            self._convolutionBlock(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 4: Upsample to 32x32\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*4, 16, 16)\n",
    "            # Output: (N, features_g*2, 32, 32)\n",
    "            self._convolutionBlock(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Final Layer: Upsample to 64x64 (Target Image Size)\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*2, 32, 32)\n",
    "            # Output: (N, channels_img, 64, 64) [Final generated image]\n",
    "            # Explanation:\n",
    "            # - The final layer **does not use BatchNorm** (per the DCGAN paper).\n",
    "            # - Uses `Tanh` activation to output values in [-1, 1] to match normalized image range.\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2,\n",
    "                out_channels=channels_img,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def _convolutionBlock(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Defines a transposed convolutional block used in the Generator.\n",
    "        Each block consists of:\n",
    "        - A Transposed Convolution (upsampling operation)\n",
    "        - Batch Normalization (to stabilize training)\n",
    "        - ReLU Activation (for non-linearity)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input feature maps.\n",
    "        out_channels : int\n",
    "            Number of output feature maps.\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel.\n",
    "        stride : int\n",
    "            Stride of the transposed convolution (typically 2 for upsampling).\n",
    "        padding : int\n",
    "            Padding applied to the transposed convolution (typically 1 for proper output size).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        nn.Sequential : A sequential block of operations.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False  # No bias since BatchNorm is used.\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),  # Stabilizes training by normalizing activations.\n",
    "            nn.ReLU()  # Activation function to introduce non-linearity.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Generator.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input noise vector of shape (N, z_dim, 1, 1).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Output image tensor of shape (N, channels_img, 64, 64).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a1a639-0420-46a3-904f-55f1673e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            # Per DCGAN paper, we initialize weights from a normal distribution with mean=0, std=0.02\n",
    "            # This helps stabilize training and prevents mode collapse in GANs.\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02) \n",
    "\n",
    "            # For batch normalization layers, these are default values but explicitly stating it again\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                # Gamma (scaling factor) is initialized to follow N(1, 0.02) per DCGAN recommendations\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)  \n",
    "\n",
    "                # Beta (bias) is set to zero, ensuring the initial batch normalization does not shift activations\n",
    "                nn.init.constant_(m.bias.data, 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6959a6-c6b8-4fbd-bd79-7db009141410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device):\n",
    "    \"\"\"\n",
    "    Computes the gradient penalty for enforcing the Lipschitz constraint in Wasserstein GANs with Gradient Penalty (WGAN-GP).\n",
    "    \n",
    "    Args:\n",
    "        critic (nn.Module): The critic (discriminator) network that maps images to real-valued scores.\n",
    "        real (torch.Tensor): A batch of real images with shape (batch_size, channels, height, width).\n",
    "        fake (torch.Tensor): A batch of generated (fake) images with the same shape as `real`.\n",
    "        device (torch.device): The device (CPU or GPU) where computations should be performed.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The gradient penalty term, a scalar tensor encouraging the gradient norm to be close to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract shape parameters\n",
    "    batch_size, channel_size, height, width = real.shape  # Expecting a 4D tensor: (N, C, H, W)\n",
    "    \n",
    "    # Generate a random interpolation factor `epsilon` for each sample in the batch.\n",
    "    # Shape: (batch_size, 1, 1, 1) -> Broadcasting ensures uniform weighting across all channels & spatial dimensions.\n",
    "    epsilon = torch.rand((batch_size, 1, 1, 1), device=device).repeat(1, channel_size, height, width)  # (N, C, H, W)\n",
    "\n",
    "    # Compute interpolated images as a convex combination of real and fake samples\n",
    "    # Each interpolated image is: ε * real + (1 - ε) * fake\n",
    "    interpolated_images = (real * epsilon) + (fake * (1 - epsilon))  # (N, C, H, W)\n",
    "\n",
    "    # Pass interpolated images through the critic to obtain output scores\n",
    "    mixed_scores = critic(interpolated_images)  # (N, 1) assuming critic outputs a single value per image\n",
    "\n",
    "    # Compute gradients of the critic scores w.r.t. the interpolated images\n",
    "    # grad now holds ∂critic/∂interpolated_images with shape (N, C, H, W)\n",
    "    grad = torch.autograd.grad(\n",
    "        inputs=interpolated_images,    # Input tensor (N, C, H, W)\n",
    "        outputs=mixed_scores,          # Scalar output tensor per sample (N, 1)\n",
    "        grad_outputs=torch.ones_like(mixed_scores),  # Backpropagation signal, same shape as `mixed_scores` (N, 1)\n",
    "        create_graph=True,             # Enables higher-order gradients (for penalty term calculation)\n",
    "        retain_graph=True,             # Retains computation graph for further operations\n",
    "    )[0] # Grad returns tuple size equal to number of inputs, for each gradient w.r.t. each inputs\n",
    "\n",
    "    # Reshape gradient tensor to collapse all non-batch dimensions into a single vector per sample\n",
    "    grad = grad.view(grad.shape[0], -1)  # (N, C*H*W)\n",
    "\n",
    "    # Compute L2 norm of gradients per sample along feature dimension\n",
    "    grad_norm = grad.norm(2, dim=1)  # (N,)\n",
    "\n",
    "    # Compute the gradient penalty term, enforcing ||∇D(interpolated_images)|| ≈ 1\n",
    "    grad_penalty = torch.mean((grad_norm - 1) ** 2)  # Scalar tensor\n",
    "\n",
    "    return grad_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97000ecc-f3bb-475f-8e2c-af9e82fc4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Test Passed\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    # N: batch size, in_channels: input channels (e.g., RGB images), H: height, W: width\n",
    "    N, in_channels, H, W = 8, 3, 64, 64 \n",
    "    # Dimensionality of the noise vector (latent space)\n",
    "    z_dim = 100  \n",
    "\n",
    "    # Input should be batch of RGB images of HxW\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc_model = Discriminator(in_channels, 8)  \n",
    "    initialize_weights(disc_model)\n",
    "    # Output should be one value indicating real or fake\n",
    "    assert disc_model(x).shape == (N, 1, 1, 1)\n",
    "\n",
    "    # Input should be latent noise with z_dim for each channel\n",
    "    gen_model = Generator(z_dim, in_channels, 8)  \n",
    "    initialize_weights(gen_model)\n",
    "    z = torch.randn((N, z_dim, 1, 1))\n",
    "    # Output should be RGB images of HxW\n",
    "    assert gen_model(z).shape == (N, in_channels, H, W)\n",
    "\n",
    "    print(\"Dimension Test Passed\")\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57d711-fe57-40e6-b1c8-5e7ebf68f077",
   "metadata": {},
   "source": [
    "# Wasserstein GAN with Gradient Penalty on CelebA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4eba71-16eb-4d72-8318-e55bae68dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters for training\n",
    "lr = 1e-4\n",
    "batch_size = 64 \n",
    "img_size = 64\n",
    "img_channels_celeb = 3\n",
    "z_dim = 100\n",
    "num_epochs = 5\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "critic_iterations = 5\n",
    "lambda_penalty = 10\n",
    "\n",
    "# Data transformations for preprocessing the CELEB dataset\n",
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        # Resize images to the specified size (img_size x img_size)\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the images so pixel values are between [-1, 1]\n",
    "        transforms.Normalize([0.5 for _ in range(img_channels_celeb)], [0.5 for _ in range(img_channels_celeb)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the CelebA dataset with the specified transformations\n",
    "dataset = datasets.ImageFolder(root=\"./dataset/CelebA\", transform=transformer)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the generator and discriminator models and move them to the appropriate device (GPU or CPU)\n",
    "gen_model = Generator(z_dim, img_channels_celeb, features_gen).to(device)\n",
    "disc_model = Discriminator(img_channels_celeb, features_disc).to(device)\n",
    "\n",
    "# Initialize the weights of both the generator and discriminator using the custom function\n",
    "initialize_weights(gen_model)\n",
    "initialize_weights(disc_model)\n",
    "\n",
    "# Set up Adam optimizers for both models with the same learning rate and betas for stability in training\n",
    "optimizer_gen = optim.Adam(gen_model.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "optimizer_disc = optim.Adam(disc_model.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "\n",
    "# Generate a fixed noise vector for visualizing the output of the generator during training\n",
    "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Set up TensorBoard writers to log images for both real and fake images\n",
    "writer_fake = SummaryWriter(f\"runs/DCGAN_CELEB/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/DCGAN_CELEB/real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2b597-08bd-454b-be7b-4999d974a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/DCGAN_CELEB --bind_all --port=6006\n",
    "print(\"Tensorboard is running on port 6006\")\n",
    "\n",
    "# Set the models to training mode\n",
    "gen_model.train()\n",
    "disc_model.train()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # Move the real images to the device (GPU/CPU)\n",
    "        real = real.to(device)\n",
    "\n",
    "        # --- Train the critic ---\n",
    "        for _ in range(critic_iterations):\n",
    "            # Generate random noise to feed into the generator\n",
    "            noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        \n",
    "            # Generate fake images using the generator\n",
    "            fake = gen_model(noise)\n",
    "        \n",
    "            # Compute the discriminator's output on real images\n",
    "            disc_real = disc_model(real).reshape(-1)\n",
    "            \n",
    "            # Compute the discriminator's output on fake images\n",
    "            disc_fake = disc_model(fake.detach()).reshape(-1)\n",
    "\n",
    "            # We want to maximize this, so negative the minimization along with gradient penalty\n",
    "            penalty = gradient_penalty(disc_model, real, fake, device)\n",
    "            loss_disc = (-(torch.mean(disc_real) - torch.mean(disc_fake))) + (lambda_penalty * penalty)\n",
    "        \n",
    "            disc_model.zero_grad()\n",
    "            loss_disc.backward(retain_graph=True)\n",
    "            optimizer_disc.step()\n",
    "\n",
    "        # --- Train the Generator ---\n",
    "        # Compute the discriminator's output on the fake images\n",
    "        output = disc_model(fake).reshape(-1)\n",
    "        \n",
    "        # min -E[critic(gen_fake)]\n",
    "        loss_gen = -torch.mean(output)\n",
    "        \n",
    "        gen_model.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Print losses and log images to TensorBoard every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Log real and fake images to TensorBoard using a fixed noise vector\n",
    "            with torch.no_grad():  # No gradients are needed for this step\n",
    "                fake = gen_model(fixed_noise)  # Generate a batch of fake images using fixed noise\n",
    "                # Create image grids for real and fake images (up to 32 images)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                # Add images to TensorBoard (real and fake images for visualization)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e13d2-b9d3-4bf3-9ffc-411d9767e937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
