{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c51f25-e8b2-4583-b4e3-29c09939f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9f62f-888e-4b4e-ae31-77d434506274",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "## Discriminator Class\n",
    "- A convolutional neural network designed to classify images as real or fake.\n",
    "- Takes in an image and outputs a probability value indicating whether the image is real or fake.\n",
    "- Uses convolutional layers with **LeakyReLU** activations and **BatchNorm** layers.\n",
    "- The final output is a single probability (between 0 and 1) for each image.\n",
    "\n",
    "## Generator Class\n",
    "- A convolutional neural network that generates synthetic images from random noise vectors.\n",
    "- Takes in a latent vector (usually a random noise vector) and outputs an image.\n",
    "- Uses transposed convolution layers to upsample the latent vector into an image of the desired size.\n",
    "- Outputs images normalized between [-1, 1] using the **Tanh** activation function.\n",
    "\n",
    "## Initialize Weights\n",
    "- A function to initialize the weights of the model layers using **normal distribution** (mean = 0, std = 0.02).\n",
    "- The **Convolutional layers** (`Conv2d` and `ConvTranspose2d`) and **Batch Normalization layers** (`BatchNorm2d`) are initialized to improve training stability.\n",
    "- For **BatchNorm layers**, the scaling factor (`gamma`) is initialized using **normal distribution** with mean = 1 and std = 0.02 to allow slight flexibility in activation scaling.\n",
    "- The **bias** of BatchNorm layers is initialized to 0 to prevent any initial shift in activations.\n",
    "\n",
    "## Simple Dimension Check\n",
    "- A function to verify that the input and output dimensions of both the **Discriminator** and **Generator** are correct.\n",
    "- Ensures that the Discriminator outputs a tensor of shape `(N, 1, 1, 1)` where `N` is the batch size.\n",
    "- Ensures that the Generator outputs a tensor of shape `(N, channels_img, H, W)` where `H` and `W` are the target image dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2abcfd-20ec-4f26-b350-6f675aa63ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        \"\"\"\n",
    "        Implements the Discriminator for DCGAN.\n",
    "        This model follows a convolutional architecture that progressively reduces the spatial dimensions \n",
    "        while increasing the feature depth. The final output is a single scalar value (0 or 1) indicating \n",
    "        whether the input image is real or fake.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        channels_img : int\n",
    "            Number of channels in the input image. \n",
    "            (For RGB images, this is typically 3. For grayscale images like MNIST, it's 1.)\n",
    "        features_d : int\n",
    "            Number of feature maps in the first convolutional layer.\n",
    "            This number scales up in deeper layers to capture more complex features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # -------------------------\n",
    "            # Layer 1: Initial Convolution\n",
    "            # -------------------------\n",
    "            # Input: (N, channels_img, 64, 64)\n",
    "            # Output: (N, features_d, 32, 32)\n",
    "            # Explanation:\n",
    "            # - Stride=2 reduces width and height by half\n",
    "            # - No batch normalization in the first layer as per the DCGAN paper\n",
    "            nn.Conv2d(\n",
    "                channels_img,      # Number of input channels (e.g., 3 for RGB images)\n",
    "                features_d,        # Number of output feature maps\n",
    "                kernel_size=4,     # 4x4 convolution kernel\n",
    "                stride=2,          # Reduces spatial dimensions (64x64 -> 32x32)\n",
    "                padding=1          # Maintains proper output size after convolution\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),  # LeakyReLU activation with a negative slope of 0.2\n",
    "\n",
    "            # -------------------------\n",
    "            # Layer 2: Downsampling\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d, 32, 32)\n",
    "            # Output: (N, features_d*2, 16, 16)\n",
    "            self._convolutionBlock(features_d, features_d*2, 4, 2, 1),\n",
    "\n",
    "            # -------------------------\n",
    "            # Layer 3: Downsampling\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d*2, 16, 16)\n",
    "            # Output: (N, features_d*4, 8, 8)\n",
    "            self._convolutionBlock(features_d*2, features_d*4, 4, 2, 1),\n",
    "\n",
    "            # -------------------------\n",
    "            # Layer 4: Downsampling\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d*4, 8, 8)\n",
    "            # Output: (N, features_d*8, 4, 4)\n",
    "            self._convolutionBlock(features_d*4, features_d*8, 4, 2, 1),\n",
    "\n",
    "            # -------------------------\n",
    "            # Final Layer: Fully Connected Convolution\n",
    "            # -------------------------\n",
    "            # Input: (N, features_d*8, 4, 4)\n",
    "            # Output: (N, 1, 1, 1)\n",
    "            # Explanation:\n",
    "            # - This layer performs a final convolution that reduces the spatial dimension to 1x1\n",
    "            # - The output is a single value per image, indicating the probability of being real or fake\n",
    "            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _convolutionBlock(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Defines a convolutional block used in the Discriminator.\n",
    "        Each block consists of:\n",
    "        - A 2D Convolution (with no bias to improve stability)\n",
    "        - Batch Normalization (to stabilize learning)\n",
    "        - LeakyReLU Activation (to allow small gradients even for negative inputs)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output feature maps\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel\n",
    "        stride : int\n",
    "            Stride of the convolution (typically 2 for downsampling)\n",
    "        padding : int\n",
    "            Padding applied to the convolution (typically 1 to maintain proper dimensions)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        nn.Sequential : A sequential block of operations\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,  # Bias is removed as BatchNorm handles normalization\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),  # Normalizes feature maps to stabilize training\n",
    "            nn.LeakyReLU(0.2),  # Allows a small gradient for negative inputs, avoiding dead neurons\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Discriminator.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input image tensor of shape (N, channels_img, 64, 64)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (N, 1, 1, 1), representing probability scores of being real/fake.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81797d1-7241-4e51-bbb4-be57bcd8fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        \"\"\"\n",
    "        Implements the Generator for DCGAN.\n",
    "        The Generator takes a random noise vector (latent space) and transforms it \n",
    "        into a realistic-looking image through a series of transposed convolutions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z_dim : int\n",
    "            Dimension of the latent noise vector (typically 100 in DCGAN implementations).\n",
    "        channels_img : int\n",
    "            Number of channels in the generated image.\n",
    "            (For RGB images, this is typically 3. For grayscale images, it's 1.)\n",
    "        features_g : int\n",
    "            Number of feature maps in the first transposed convolutional layer.\n",
    "            This number scales down in deeper layers to generate finer details.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # ---------------------------------------\n",
    "            # Layer 1: Transform Noise Vector (z_dim) into Feature Maps\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, z_dim, 1, 1)  [Latent space input]\n",
    "            # Output: (N, features_g*16, 4, 4)\n",
    "            # Explanation:\n",
    "            # - Converts the 1x1 noise vector into a 4x4 feature map.\n",
    "            # - `stride=1` and `padding=0` ensure the output starts as exactly 4x4.\n",
    "            self._convolutionBlock(z_dim, features_g*16, kernel_size=4, stride=1, padding=0),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 2: Upsample to 8x8\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*16, 4, 4)\n",
    "            # Output: (N, features_g*8, 8, 8)\n",
    "            self._convolutionBlock(features_g*16, features_g*8, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 3: Upsample to 16x16\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*8, 8, 8)\n",
    "            # Output: (N, features_g*4, 16, 16)\n",
    "            self._convolutionBlock(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Layer 4: Upsample to 32x32\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*4, 16, 16)\n",
    "            # Output: (N, features_g*2, 32, 32)\n",
    "            self._convolutionBlock(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # Final Layer: Upsample to 64x64 (Target Image Size)\n",
    "            # ---------------------------------------\n",
    "            # Input: (N, features_g*2, 32, 32)\n",
    "            # Output: (N, channels_img, 64, 64) [Final generated image]\n",
    "            # Explanation:\n",
    "            # - The final layer **does not use BatchNorm** (per the DCGAN paper).\n",
    "            # - Uses `Tanh` activation to output values in [-1, 1] to match normalized image range.\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2,\n",
    "                out_channels=channels_img,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def _convolutionBlock(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Defines a transposed convolutional block used in the Generator.\n",
    "        Each block consists of:\n",
    "        - A Transposed Convolution (upsampling operation)\n",
    "        - Batch Normalization (to stabilize training)\n",
    "        - ReLU Activation (for non-linearity)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input feature maps.\n",
    "        out_channels : int\n",
    "            Number of output feature maps.\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel.\n",
    "        stride : int\n",
    "            Stride of the transposed convolution (typically 2 for upsampling).\n",
    "        padding : int\n",
    "            Padding applied to the transposed convolution (typically 1 for proper output size).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        nn.Sequential : A sequential block of operations.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False  # No bias since BatchNorm is used.\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),  # Stabilizes training by normalizing activations.\n",
    "            nn.ReLU()  # Activation function to introduce non-linearity.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Generator.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input noise vector of shape (N, z_dim, 1, 1).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Output image tensor of shape (N, channels_img, 64, 64).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a1a639-0420-46a3-904f-55f1673e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            # Per DCGAN paper, we initialize weights from a normal distribution with mean=0, std=0.02\n",
    "            # This helps stabilize training and prevents mode collapse in GANs.\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02) \n",
    "\n",
    "            # For batch normalization layers, these are default values but explicitly stating it again\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                # Gamma (scaling factor) is initialized to follow N(1, 0.02) per DCGAN recommendations\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)  \n",
    "\n",
    "                # Beta (bias) is set to zero, ensuring the initial batch normalization does not shift activations\n",
    "                nn.init.constant_(m.bias.data, 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97000ecc-f3bb-475f-8e2c-af9e82fc4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Test Passed\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    # N: batch size, in_channels: input channels (e.g., RGB images), H: height, W: width\n",
    "    N, in_channels, H, W = 8, 3, 64, 64 \n",
    "    # Dimensionality of the noise vector (latent space)\n",
    "    z_dim = 100  \n",
    "\n",
    "    # Input should be batch of RGB images of HxW\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc_model = Discriminator(in_channels, 8)  \n",
    "    initialize_weights(disc_model)\n",
    "    # Output should be one value indicating real or fake\n",
    "    assert disc_model(x).shape == (N, 1, 1, 1)\n",
    "\n",
    "    # Input should be latent noise with z_dim for each channel\n",
    "    gen_model = Generator(z_dim, in_channels, 8)  \n",
    "    initialize_weights(gen_model)\n",
    "    z = torch.randn((N, z_dim, 1, 1))\n",
    "    # Output should be RGB images of HxW\n",
    "    assert gen_model(z).shape == (N, in_channels, H, W)\n",
    "\n",
    "    print(\"Dimension Test Passed\")\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57d711-fe57-40e6-b1c8-5e7ebf68f077",
   "metadata": {},
   "source": [
    "# Deep Convolutional GAN on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4eba71-16eb-4d72-8318-e55bae68dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters for training\n",
    "lr = 2e-4 \n",
    "batch_size = 128 \n",
    "img_size = 64\n",
    "img_channels_mnist = 1\n",
    "z_dim = 100\n",
    "num_epochs = 5\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "\n",
    "# Data transformations for preprocessing the MNIST dataset\n",
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        # Resize images to the specified size (img_size x img_size)\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the images so pixel values are between [-1, 1]\n",
    "        transforms.Normalize([0.5 for _ in range(img_channels_mnist)], [0.5 for _ in range(img_channels_mnist)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the MNIST dataset with the specified transformations\n",
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transformer, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the generator and discriminator models and move them to the appropriate device (GPU or CPU)\n",
    "gen_model = Generator(z_dim, img_channels_mnist, features_gen).to(device)\n",
    "disc_model = Discriminator(img_channels_mnist, features_disc).to(device)\n",
    "\n",
    "# Initialize the weights of both the generator and discriminator using the custom function\n",
    "initialize_weights(gen_model)\n",
    "initialize_weights(disc_model)\n",
    "\n",
    "# Set up Adam optimizers for both models with the same learning rate and betas for stability in training\n",
    "optimizer_gen = optim.Adam(gen_model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_disc = optim.Adam(disc_model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Binary Cross-Entropy loss (BCELoss) for adversarial training\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Generate a fixed noise vector for visualizing the output of the generator during training\n",
    "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Set up TensorBoard writers to log images for both real and fake images\n",
    "writer_fake = SummaryWriter(f\"runs/DCGAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/DCGAN_MNIST/real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c2b597-08bd-454b-be7b-4999d974a542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5463), started 0:00:48 ago. (Use '!kill 5463' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f4fc426c1becfaa9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f4fc426c1becfaa9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard is running on port 6006\n",
      "Epoch [0/5] Batch 0/469                   Loss D: 1.0604, loss G: 6.9379\n",
      "Epoch [0/5] Batch 100/469                   Loss D: 0.2769, loss G: 2.8675\n",
      "Epoch [0/5] Batch 200/469                   Loss D: 0.5650, loss G: 3.9891\n",
      "Epoch [0/5] Batch 300/469                   Loss D: 0.3816, loss G: 2.2462\n",
      "Epoch [0/5] Batch 400/469                   Loss D: 0.3516, loss G: 1.4641\n",
      "Epoch [1/5] Batch 0/469                   Loss D: 0.4180, loss G: 2.2748\n",
      "Epoch [1/5] Batch 100/469                   Loss D: 0.5102, loss G: 2.3008\n",
      "Epoch [1/5] Batch 200/469                   Loss D: 0.4867, loss G: 4.2233\n",
      "Epoch [1/5] Batch 300/469                   Loss D: 0.3670, loss G: 1.0248\n",
      "Epoch [1/5] Batch 400/469                   Loss D: 0.5637, loss G: 2.1484\n",
      "Epoch [2/5] Batch 0/469                   Loss D: 0.4234, loss G: 2.5906\n",
      "Epoch [2/5] Batch 100/469                   Loss D: 0.3628, loss G: 1.6961\n",
      "Epoch [2/5] Batch 200/469                   Loss D: 0.3553, loss G: 1.3084\n",
      "Epoch [2/5] Batch 300/469                   Loss D: 0.2263, loss G: 2.7737\n",
      "Epoch [2/5] Batch 400/469                   Loss D: 0.2635, loss G: 2.1469\n",
      "Epoch [3/5] Batch 0/469                   Loss D: 0.3468, loss G: 1.5498\n",
      "Epoch [3/5] Batch 100/469                   Loss D: 0.2709, loss G: 4.1807\n",
      "Epoch [3/5] Batch 200/469                   Loss D: 0.4334, loss G: 2.1763\n",
      "Epoch [3/5] Batch 300/469                   Loss D: 0.4399, loss G: 6.0343\n",
      "Epoch [3/5] Batch 400/469                   Loss D: 0.4815, loss G: 5.1058\n",
      "Epoch [4/5] Batch 0/469                   Loss D: 0.2147, loss G: 2.5509\n",
      "Epoch [4/5] Batch 100/469                   Loss D: 2.3963, loss G: 0.3535\n",
      "Epoch [4/5] Batch 200/469                   Loss D: 0.0723, loss G: 3.4290\n",
      "Epoch [4/5] Batch 300/469                   Loss D: 0.2328, loss G: 2.6934\n",
      "Epoch [4/5] Batch 400/469                   Loss D: 0.9389, loss G: 3.6521\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=runs/DCGAN_MNIST --bind_all --port=6006\n",
    "print(\"Tensorboard is running on port 6006\")\n",
    "\n",
    "# Set the models to training mode\n",
    "gen_model.train()\n",
    "disc_model.train()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # Move the real images to the device (GPU/CPU)\n",
    "        real = real.to(device)\n",
    "        \n",
    "        # Generate random noise to feed into the generator\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        \n",
    "        # Generate fake images using the generator\n",
    "        fake = gen_model(noise)\n",
    "\n",
    "        # --- Train the Discriminator ---\n",
    "        # Compute the discriminator's output on real images\n",
    "        disc_real = disc_model(real).reshape(-1)\n",
    "        # Calculate the discriminator's loss on real images (maximize log(D(x)))\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        \n",
    "        # Compute the discriminator's output on fake images\n",
    "        disc_fake = disc_model(fake.detach()).reshape(-1)\n",
    "        # Calculate the discriminator's loss on fake images (maximize log(1 - D(G(z))))\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        # Total discriminator loss is the average of the real and fake losses\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        \n",
    "        disc_model.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # --- Train the Generator ---\n",
    "        # Compute the discriminator's output on the fake images\n",
    "        output = disc_model(fake).reshape(-1)\n",
    "        \n",
    "        # Generator's objective is to fool the discriminator, so it minimizes log(1 - D(G(z))) \n",
    "        # This is equivalent to maximizing log(D(G(z))) in practice.\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        \n",
    "        gen_model.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Print losses and log images to TensorBoard every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Log real and fake images to TensorBoard using a fixed noise vector\n",
    "            with torch.no_grad():  # No gradients are needed for this step\n",
    "                fake = gen_model(fixed_noise)  # Generate a batch of fake images using fixed noise\n",
    "                # Create image grids for real and fake images (up to 32 images)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                # Add images to TensorBoard (real and fake images for visualization)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee11eb-31ae-43d5-8e1e-d660449096af",
   "metadata": {},
   "source": [
    "# Deep Convolutional GAN on CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "620ed5d8-82b3-4ced-8809-834915633131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CelebA dataset is RGB image\n",
    "img_channels_celeb = 3\n",
    "\n",
    "# Data transformations for preprocessing the CELEB dataset\n",
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        # Resize images to the specified size (img_size x img_size)\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the images so pixel values are between [-1, 1]\n",
    "        transforms.Normalize([0.5 for _ in range(img_channels_celeb)], [0.5 for _ in range(img_channels_celeb)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the CELEB dataset with the specified transformations\n",
    "dataset = datasets.ImageFolder(root=\"./dataset/CelebA\", transform=transformer)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the generator and discriminator models and move them to the appropriate device (GPU or CPU)\n",
    "gen_model = Generator(z_dim, img_channels_celeb, features_gen).to(device)\n",
    "disc_model = Discriminator(img_channels_celeb, features_disc).to(device)\n",
    "\n",
    "# Initialize the weights of both the generator and discriminator using the custom function\n",
    "initialize_weights(gen_model)\n",
    "initialize_weights(disc_model)\n",
    "\n",
    "# Set up Adam optimizers for both models with the same learning rate and betas for stability in training\n",
    "optimizer_gen = optim.Adam(gen_model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_disc = optim.Adam(disc_model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Binary Cross-Entropy loss (BCELoss) for adversarial training\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Generate a fixed noise vector for visualizing the output of the generator during training\n",
    "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Set up TensorBoard writers to log images for both real and fake images\n",
    "writer_fake = SummaryWriter(f\"runs/DCGAN_CELEB/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/DCGAN_CELEB/real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "368bca96-3b4a-4eac-ad84-f3302f370a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a757c2ca69ff0b78\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a757c2ca69ff0b78\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard is running on port 6006\n",
      "Epoch [0/5] Batch 0/1583                   Loss D: 1.0034, loss G: 7.4550\n",
      "Epoch [0/5] Batch 100/1583                   Loss D: 0.3456, loss G: 4.4890\n",
      "Epoch [0/5] Batch 200/1583                   Loss D: 0.5005, loss G: 4.0865\n",
      "Epoch [0/5] Batch 300/1583                   Loss D: 0.5288, loss G: 4.0607\n",
      "Epoch [0/5] Batch 400/1583                   Loss D: 0.4331, loss G: 2.2625\n",
      "Epoch [0/5] Batch 500/1583                   Loss D: 0.4269, loss G: 2.6272\n",
      "Epoch [0/5] Batch 600/1583                   Loss D: 0.4612, loss G: 2.5881\n",
      "Epoch [0/5] Batch 700/1583                   Loss D: 0.8033, loss G: 3.0244\n",
      "Epoch [0/5] Batch 800/1583                   Loss D: 0.6337, loss G: 3.0412\n",
      "Epoch [0/5] Batch 900/1583                   Loss D: 0.4623, loss G: 2.9223\n",
      "Epoch [0/5] Batch 1000/1583                   Loss D: 0.5819, loss G: 1.6727\n",
      "Epoch [0/5] Batch 1100/1583                   Loss D: 0.4537, loss G: 2.1151\n",
      "Epoch [0/5] Batch 1200/1583                   Loss D: 0.4547, loss G: 1.6811\n",
      "Epoch [0/5] Batch 1300/1583                   Loss D: 0.4881, loss G: 2.3860\n",
      "Epoch [0/5] Batch 1400/1583                   Loss D: 0.4296, loss G: 1.6861\n",
      "Epoch [0/5] Batch 1500/1583                   Loss D: 0.4096, loss G: 2.0116\n",
      "Epoch [1/5] Batch 0/1583                   Loss D: 0.4500, loss G: 1.9521\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move the real images to the device (GPU/CPU)\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Generate random noise to feed into the generator\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torchvision/datasets/folder.py:262\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pkill -f \"tensorboard\"\n",
    "%tensorboard --logdir=runs/DCGAN_CELEB --bind_all --port=6006\n",
    "print(\"Tensorboard is running on port 6006\")\n",
    "\n",
    "# Set the models to training mode\n",
    "gen_model.train()\n",
    "disc_model.train()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # Move the real images to the device (GPU/CPU)\n",
    "        real = real.to(device)\n",
    "        \n",
    "        # Generate random noise to feed into the generator\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        \n",
    "        # Generate fake images using the generator\n",
    "        fake = gen_model(noise)\n",
    "\n",
    "        # --- Train the Discriminator ---\n",
    "        # Compute the discriminator's output on real images\n",
    "        disc_real = disc_model(real).reshape(-1)\n",
    "        # Calculate the discriminator's loss on real images (maximize log(D(x)))\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        \n",
    "        # Compute the discriminator's output on fake images\n",
    "        disc_fake = disc_model(fake.detach()).reshape(-1)\n",
    "        # Calculate the discriminator's loss on fake images (maximize log(1 - D(G(z))))\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        # Total discriminator loss is the average of the real and fake losses\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        \n",
    "        disc_model.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # --- Train the Generator ---\n",
    "        # Compute the discriminator's output on the fake images\n",
    "        output = disc_model(fake).reshape(-1)\n",
    "        \n",
    "        # Generator's objective is to fool the discriminator, so it minimizes log(1 - D(G(z))) \n",
    "        # This is equivalent to maximizing log(D(G(z))) in practice.\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        \n",
    "        gen_model.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Print losses and log images to TensorBoard every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Log real and fake images to TensorBoard using a fixed noise vector\n",
    "            with torch.no_grad():  # No gradients are needed for this step\n",
    "                fake = gen_model(fixed_noise)  # Generate a batch of fake images using fixed noise\n",
    "                # Create image grids for real and fake images (up to 32 images)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                # Add images to TensorBoard (real and fake images for visualization)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e13d2-b9d3-4bf3-9ffc-411d9767e937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
