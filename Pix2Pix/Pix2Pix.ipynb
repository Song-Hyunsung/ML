{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa4c59-128b-4a5d-8108-6778a99946e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Save/Load Checkpoint\n",
    "# Save/Load Model\n",
    "# Look into training optimization\n",
    "# - GradScaler\n",
    "# - NumWorkers during loading\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0058f9-7c1c-4d3e-87e2-a197735a095b",
   "metadata": {},
   "source": [
    "# Discriminator Architecture (Pix2Pix)\n",
    "\n",
    "The `Discriminator` class implements a **PatchGAN-based** discriminator, commonly used in the **Pix2Pix** model for image-to-image translation. It classifies whether each patch in an image is real or fake.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "- The discriminator takes as input a **pair of images** `(x, y)`, where:\n",
    "  - `x` is the input image.\n",
    "  - `y` is the target/generated image.\n",
    "  - These two images are concatenated along the channel dimension (`dim=1`).\n",
    "  \n",
    "- It consists of:\n",
    "  1. **An Initial Convolution Layer**:\n",
    "     - Uses a `Conv2d` layer with:\n",
    "       - Input channels: `in_channels * 2` (since both `x` and `y` are concatenated).\n",
    "       - Output channels: `features[0]` (typically `64`).\n",
    "       - Kernel size: `4x4`.\n",
    "       - Stride: `2` (reduces spatial dimensions).\n",
    "       - No Batch Normalization (as in the Pix2Pix paper).\n",
    "       - Activation: `LeakyReLU(0.2)`.\n",
    "  \n",
    "  2. **A Series of CNN Blocks (`CNNBlock`)**:\n",
    "     - Each block consists of:\n",
    "       - A `Conv2d` layer with **Batch Normalization** and **LeakyReLU(0.2)**.\n",
    "       - Stride is `2` for all layers **except** the last one.\n",
    "       - Channels progress as defined in `features=[64, 128, 256, 512]`.\n",
    "  \n",
    "  3. **Final Convolution Layer**:\n",
    "     - A single **`Conv2d`** layer with:\n",
    "       - Output channels = `1` (discriminator outputs a single-channel probability map).\n",
    "       - Kernel size: `4x4`.\n",
    "       - Stride: `1`.\n",
    "       - Padding: `1`.\n",
    "       - No activation function (raw logits output).\n",
    "\n",
    "## Notes\n",
    "\n",
    "- The discriminator **does not output a single value** but rather a **feature map**, where each patch's value represents its real/fake probability.\n",
    "- The `padding_mode=\"reflect\"` is used to **reduce artifacts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bd15e70-9c23-43c4-b173-d9256cb3b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False, padding_mode=\"reflect\"),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            # Channels are doubled because in Pix2Pix model, the discriminator takes (x,y) pair as input\n",
    "            # Initial layer does not perform batch normalization and hence specified separately\n",
    "            nn.Conv2d(in_channels*2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        convolution_layers = []\n",
    "        in_channels = features[0]\n",
    "        \n",
    "        for feature in features[1:]:\n",
    "            convolution_layers.append(\n",
    "                # Stride is 2 except for the last layer\n",
    "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        convolution_layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(*convolution_layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Concatenate x and y along the channel\n",
    "        x = torch.cat([x,y], dim=1)\n",
    "        x = self.initial(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0442d46-7033-4f09-b898-aa2feb1f8fc9",
   "metadata": {},
   "source": [
    "# Generator Architecture (UNet-based)\n",
    "\n",
    "The `Generator` class implements a **UNet-based** architecture, commonly used in **Pix2Pix** for image-to-image translation. It follows an **encoder-decoder** structure with **skip connections** to retain spatial information lost during downsampling.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The generator consists of the following components:\n",
    "\n",
    "1. **Initial Downsampling Block**:\n",
    "   - A `Conv2d` layer with:\n",
    "     - Input channels: `in_channels` (default: `3` for RGB).\n",
    "     - Output channels: `features` (default: `64`).\n",
    "     - Kernel size: `4x4`, stride `2`, padding `1`.\n",
    "     - `LeakyReLU(0.2)` activation.\n",
    "   - No batch normalization is applied in this layer.\n",
    "\n",
    "2. **Series of Encoder Blocks**:\n",
    "   - Each encoder block (`UNetBlock`) performs **downsampling** using:\n",
    "     - A `Conv2d` layer with `stride=2` to reduce spatial dimensions.\n",
    "     - Batch Normalization.\n",
    "     - `LeakyReLU(0.2)` activation.\n",
    "   - Skip connections store intermediate outputs for later use in the decoder.\n",
    "\n",
    "3. **Bottleneck Layer**:\n",
    "   - A `Conv2d` layer with:\n",
    "     - Kernel size: `4x4`, stride `2`, padding `1`.\n",
    "     - `ReLU` activation (no batch normalization).\n",
    "   - A **bottleneck upsampling** block follows, performing:\n",
    "     - Transposed convolution (`ConvTranspose2d`).\n",
    "     - `ReLU` activation.\n",
    "     - **Dropout (`0.5`)** is applied to this layer.\n",
    "\n",
    "4. **Series of Decoder Blocks**:\n",
    "   - Each decoder block (`UNetBlock`) performs **upsampling** using:\n",
    "     - A `ConvTranspose2d` layer with `stride=2` to increase spatial dimensions.\n",
    "     - Batch Normalization.\n",
    "     - `ReLU` activation.\n",
    "     - **Dropout** applied for the first `decoder_dropout_range` layers.\n",
    "   - Decoder blocks take input from both:\n",
    "     - The previous decoder layer.\n",
    "     - The **corresponding encoder output** (via **skip connections**).\n",
    "\n",
    "5. **Final Upsampling Block**:\n",
    "   - A `ConvTranspose2d` layer with:\n",
    "     - Output channels: `in_channels` (matching input size).\n",
    "     - `Tanh` activation to normalize output values between `[-1, 1]`.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- **Skip Connections** help retain high-resolution details lost during downsampling.\n",
    "- The **use of dropout** in certain decoder layers acts as regularization.\n",
    "- The generator **learns a mapping** from input images (`x`) to target images (`y`), making it suitable for **image-to-image translation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27e0503e-e62c-4522-8e9a-bb9dd3131d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_sample=True, activation=\"relu\", use_dropout=False):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        if down_sample:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\"))\n",
    "        else:\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False))\n",
    "            \n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            \n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "            \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=64, encoder_blocks=[], decoder_blocks=[], decoder_dropout_range=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial downward block with no batch normalization\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Series of downsampling encoders\n",
    "        self.encoders = nn.ModuleList()\n",
    "        for i, o in encoder_blocks:\n",
    "            self.encoders.append(\n",
    "                UNetBlock(features*i, features*o, down_sample=True, activation=\"leaky\", use_dropout=False)\n",
    "            )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.bottleneck_up = nn.Sequential(\n",
    "            UNetBlock(features*8, features*8, down_sample=False, activation=\"relu\", use_dropout=True)\n",
    "        )\n",
    "\n",
    "        # Series of upsampling decoders\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for idx, (i, o) in enumerate(decoder_blocks):\n",
    "            use_dropout = idx < decoder_dropout_range\n",
    "            self.decoders.append(\n",
    "                # Multiplied by two to account for skip connection concatenation\n",
    "                UNetBlock(features*i*2, features*o, down_sample=False, activation=\"relu\", use_dropout=use_dropout)\n",
    "            )\n",
    "\n",
    "        # Final unsampling block\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, in_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        d = self.initial(x)\n",
    "        skip_connections.append(d)\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            d = encoder(d)\n",
    "            skip_connections.append(d)\n",
    "\n",
    "        bottleneck = self.bottleneck(d)\n",
    "        u = self.bottleneck_up(bottleneck)\n",
    "\n",
    "        # Reverse the skip connections to match encoder-decoder\n",
    "        for decoder, skip_encoder in zip(self.decoders, list(reversed(skip_connections))[:-1]):\n",
    "            u = decoder(torch.cat([u, skip_encoder], dim=1))\n",
    "\n",
    "        # Connect top level encoder with decoder\n",
    "        return self.final(torch.cat([u, skip_connections[0]], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fcb2d272-14f0-4372-95bf-74e28e079430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    x = torch.randn((1, 3, 256, 256))\n",
    "    model = Generator(in_channels=3, features=64, encoder_blocks=[(1,2), (2,4), (4,8), (8,8), (8,8), (8,8)], decoder_blocks=[(8,8), (8,8), (8,8), (8,4), (4,2), (2,1)], decoder_dropout_range=2)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0d98b12c-ceb3-4f5b-b02c-8abf13344f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_lists = os.listdir(self.root_dir)\n",
    "        self.transformer = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_lists)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.file_lists[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        img = Image.open(img_path)\n",
    "        img_tensor = self.transformer(img)\n",
    "\n",
    "        # Define the resize transformation\n",
    "        resize_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256))  # Resize to 256x256\n",
    "        ])\n",
    "\n",
    "        # Split the image into img_x and img_y\n",
    "        img_x = img_tensor[:, :, :600]  # First 600 pixels along the width\n",
    "        img_y = img_tensor[:, :, 600:]  # Remaining pixels from 600 onward\n",
    "        \n",
    "        # Apply the transformation to both images\n",
    "        img_x_resized = resize_transform(img_x)\n",
    "        img_y_resized = resize_transform(img_y)\n",
    "        \n",
    "        # augmentation can be performed here if needed\n",
    "\n",
    "        return img_x_resized, img_y_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b5185665-7bd6-4bd9-a618-187cc093f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_some_examples(gen, val_loader, epoch, folder):\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aeb1d-630a-40ba-8882-1b64be7cd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_file_dir = \"./dataset/train\"\n",
    "val_file_dir = \"./dataset/val\"\n",
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "img_size = 256\n",
    "img_channels = 3\n",
    "l1_lambda = 100\n",
    "lambda_gp = 10\n",
    "num_epochs = 500\n",
    "\n",
    "disc = Discriminator(\n",
    "    in_channels=3,\n",
    "    features=[64, 128, 256, 512]\n",
    ").to(device)\n",
    "gen = Generator(\n",
    "    in_channels=3,\n",
    "    features=64,\n",
    "    encoder_blocks=[(1,2), (2,4), (4,8), (8,8), (8,8), (8,8)],\n",
    "    decoder_blocks=[(8,8), (8,8), (8,8), (8,4), (4,2), (2,1)],\n",
    "    decoder_dropout_range=2\n",
    ").to(device)\n",
    "\n",
    "disc_opt = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "gen_opt = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Wasserstein produced bad result with PatchGAN\n",
    "BCE_loss = nn.BCEWithLogitsLoss()\n",
    "L1_loss = nn.L1Loss()\n",
    "\n",
    "train_dataset = MapDataset(root_dir=train_file_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = MapDataset(root_dir=val_file_dir)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "\n",
    "    for i, (x,y) in enumerate(loop):\n",
    "        x, y = x.to(device).float(), y.to(device).float()\n",
    "\n",
    "        # Train Discriminator\n",
    "        y_fake = gen(x)\n",
    "        d_real = disc(x, y)\n",
    "        d_fake = disc(x, y_fake.detach())\n",
    "        d_real_loss = BCE_loss(d_real, torch.ones_like(d_real))\n",
    "        d_fake_loss = BCE_loss(d_fake, torch.zeros_like(d_fake))\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        disc_opt.step()\n",
    "\n",
    "        # Train Generator\n",
    "        d_fake = disc(x, y_fake)\n",
    "        g_fake_loss = BCE_loss(d_fake, torch.ones_like(d_fake))\n",
    "        l1 = L1_loss(y_fake, y) * l1_lambda\n",
    "        g_loss = g_fake_loss + l1\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "    save_some_examples(gen, val_loader, epoch, folder=\"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33419232-0640-49b1-993b-2c39db4d3e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908aadf-5f34-4eda-8182-91135af22e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
